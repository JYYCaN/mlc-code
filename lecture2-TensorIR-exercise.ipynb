{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorIR 练习"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "import numpy as np\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一节：如何编写 TensorIR\n",
    "在本节中，让我们尝试根据高级指令（例如 Numpy 或 Torch）手动编写 TensorIR。首先，我们给出一个逐位相加函数的例子，来展示我们应该如何编写一个 TensorIR 函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 示例：逐位相加\n",
    "首先，让我们尝试使用 Numpy 编写一个逐位相加函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(16, 0, -1).reshape(4, 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在我们直接编写 TensorIR 之前，我们应该首先将高级计算抽象（例如，ndarray + ndarray）转换为低级 Python 实现（具有元素访问和操作的循环的标准）。\n",
    "\n",
    "值得注意的是，输出数组（或缓冲区）的初始值并不总是 0。我们需要在我们的实现中编写或初始化它，这对于归约运算符（例如 matmul 和 conv）很重要。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low level numpy\n",
    "def low_level_np_add(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            C[i, j] = A[i, j] + B[i, j]\n",
    "c_lnumpy = np.empty((4, 4), dtype=\"int64\")\n",
    "low_level_np_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在，让我们更进一步：将低级 NumPy 实现转换为 TensorIR，并将结果与来自 NumPy 的结果进行比较。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.nd.NDArray shape=(4, 4), cpu(0)>\n",
       "array([[16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "    @T.prim_func\n",
    "    def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "            B: T.Buffer[(4, 4), \"int64\"],\n",
    "            C: T.Buffer[(4, 4), \"int64\"]):\n",
    "        T.func_attr({\"global_symbol\":\"add\"})\n",
    "        for i, j in T.grid(4, 4):\n",
    "            with T.block(\"C\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "                C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "rt_build = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_nd = tvm.nd.array(a)\n",
    "b_nd = tvm.nd.array(b)\n",
    "c_nd = tvm.nd.empty((4, 4), dtype=\"int64\")\n",
    "rt_build[\"add\"](a_nd, b_nd, c_nd)\n",
    "c_nd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 1：广播加法\n",
    "请编写一个 TensorIR 函数，将两个数组以广播的方式相加。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# init data\n",
    "a = np.arange(16).reshape(4, 4)\n",
    "b = np.arange(4, 0, -1).reshape(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# numpy version\n",
    "c_np = a + b\n",
    "c_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low level numpy\n",
    "def low_level_np_brocast_add(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    for i in range(4):\n",
    "        for j in range(4):\n",
    "            C[i, j] = A[i, j] + B[j]\n",
    "\n",
    "c_lnumpy = np.empty((4, 4), dtype=np.int64)\n",
    "low_level_np_brocast_add(a, b, c_lnumpy)\n",
    "c_lnumpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.nd.NDArray shape=(4, 4), cpu(0)>\n",
       "array([[ 4,  4,  4,  4],\n",
       "       [ 8,  8,  8,  8],\n",
       "       [12, 12, 12, 12],\n",
       "       [16, 16, 16, 16]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "    @T.prim_func\n",
    "    # -------------------------- YOUR CODE -----------------------------\n",
    "    def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "          B: T.Buffer[(4,  ), \"int64\"],\n",
    "          C: T.Buffer[(4, 4), \"int64\"]):\n",
    "        T.func_attr({\"global_symbol\": \"add\", \"tir.noalias\": True})\n",
    "        for i, j in T.grid(4, 4):\n",
    "            with T.block(\"C\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", (i, j))\n",
    "                C[vi, vj] = A[vi, vj] + B[vj]\n",
    "    # -------------------------- END OF YOUR CODE -----------------------------\n",
    "    \n",
    "rt_lib = tvm.build(MyAdd, target=\"llvm\")\n",
    "a_tvm = tvm.nd.array(a)\n",
    "b_tvm = tvm.nd.array(b)\n",
    "c_tvm = tvm.nd.array(np.empty((4, 4), dtype=np.int64))\n",
    "rt_lib[\"add\"](a_tvm, b_tvm, c_tvm)\n",
    "np.testing.assert_allclose(c_tvm.numpy(), c_np, rtol=1e-5)\n",
    "c_tvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 练习 2：二维卷积\n",
    "stride = 1, padding = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[[[ 0  1  2  3  4  5  6  7]\n",
      "   [ 8  9 10 11 12 13 14 15]\n",
      "   [16 17 18 19 20 21 22 23]\n",
      "   [24 25 26 27 28 29 30 31]\n",
      "   [32 33 34 35 36 37 38 39]\n",
      "   [40 41 42 43 44 45 46 47]\n",
      "   [48 49 50 51 52 53 54 55]\n",
      "   [56 57 58 59 60 61 62 63]]]]\n",
      "[[[[ 0  1  2]\n",
      "   [ 3  4  5]\n",
      "   [ 6  7  8]]]\n",
      "\n",
      "\n",
      " [[[ 9 10 11]\n",
      "   [12 13 14]\n",
      "   [15 16 17]]]]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "    N: batch size\n",
    "    CI: Image input channel\n",
    "    H: 图片 hight\n",
    "    W: 图片 width\n",
    "    CO: Image output channel\n",
    "\"\"\"\n",
    "\n",
    "N, CI, H, W, CO, K = 1, 1, 8, 8, 2, 3\n",
    "OUT_H, OUT_W = H - K + 1, W - K + 1\n",
    "data = np.arange(N*CI*H*W).reshape(N, CI, H, W)\n",
    "weight = np.arange(CO*CI*K*K).reshape(CO, CI, K, K)\n",
    "print(data)\n",
    "print(weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hyj/anaconda3/envs/pytorch/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[[ 474,  510,  546,  582,  618,  654],\n",
       "         [ 762,  798,  834,  870,  906,  942],\n",
       "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
       "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
       "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
       "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
       "\n",
       "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
       "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
       "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
       "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
       "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
       "         [5883, 6000, 6117, 6234, 6351, 6468]]]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch version\n",
    "import torch\n",
    "\n",
    "data_torch = torch.Tensor(data)\n",
    "weight_torch = torch.Tensor(weight)\n",
    "res_torch = torch.nn.functional.conv2d(data_torch, weight_torch)\n",
    "res_torch = res_torch.numpy().astype(np.int64)\n",
    "res_torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[[ 474,  510,  546,  582,  618,  654],\n",
       "         [ 762,  798,  834,  870,  906,  942],\n",
       "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
       "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
       "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
       "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
       "\n",
       "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
       "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
       "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
       "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
       "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
       "         [5883, 6000, 6117, 6234, 6351, 6468]]]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# low level numpy\n",
    "def low_level_np_conv2d(data: np.ndarray, weight: np.ndarray, res: np.ndarray):\n",
    "    \"\"\"\n",
    "        data shape: (N, CI, H, W)\n",
    "        weight shape: (CO, CI, K, K)\n",
    "        res shape: (N, CO, H - K + 1, W - K + 1)\n",
    "    \"\"\"\n",
    "    for N_ind in range(N):\n",
    "        for oc_ind in range(CO):\n",
    "            for h_ind in range(OUT_H):\n",
    "                for w_ind in range(OUT_W):\n",
    "                    \n",
    "                    for ci_ind in range(CI):\n",
    "                        for h_K_ind in range(K):\n",
    "                            for w_K_ind in range(K):\n",
    "                                if ci_ind == 0 and h_K_ind == 0 and w_K_ind == 0:\n",
    "                                    res[N_ind, oc_ind, h_ind, w_ind] = 0\n",
    "                                res[N_ind, oc_ind, h_ind, w_ind] = res[N_ind, oc_ind, h_ind, w_ind] + data[N_ind, ci_ind, h_K_ind + h_ind, w_K_ind + w_ind] * weight[oc_ind, ci_ind, h_K_ind, w_K_ind]\n",
    "data_np = data_torch.numpy()\n",
    "weight_np = weight_torch.numpy()\n",
    "res_np = np.empty((N, CO, H - K + 1, W - K + 1), dtype=\"int64\")\n",
    "low_level_np_conv2d(data_np, weight_np, res_np)\n",
    "res_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tvm.nd.NDArray shape=(1, 2, 6, 6), cpu(0)>\n",
       "array([[[[ 474,  510,  546,  582,  618,  654],\n",
       "         [ 762,  798,  834,  870,  906,  942],\n",
       "         [1050, 1086, 1122, 1158, 1194, 1230],\n",
       "         [1338, 1374, 1410, 1446, 1482, 1518],\n",
       "         [1626, 1662, 1698, 1734, 1770, 1806],\n",
       "         [1914, 1950, 1986, 2022, 2058, 2094]],\n",
       "\n",
       "        [[1203, 1320, 1437, 1554, 1671, 1788],\n",
       "         [2139, 2256, 2373, 2490, 2607, 2724],\n",
       "         [3075, 3192, 3309, 3426, 3543, 3660],\n",
       "         [4011, 4128, 4245, 4362, 4479, 4596],\n",
       "         [4947, 5064, 5181, 5298, 5415, 5532],\n",
       "         [5883, 6000, 6117, 6234, 6351, 6468]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO (prim func 的参数问题，能不能使用变量)\n",
    "# TVMScript\n",
    "@tvm.script.ir_module\n",
    "class MyConv:\n",
    "    @T.prim_func\n",
    "    def conv(data: T.Buffer[(N, CI, 8, 8), \"int64\"],\n",
    "             weight: T.Buffer[(CO, CI, K, K), \"int64\"],\n",
    "             res: T.Buffer[(N, CO, OUT_H, OUT_W), \"int64\"]):\n",
    "        T.func_attr({\"global_symbol\": \"conv\", \"tir.noalias\": True})\n",
    "        for N_ind, oc_ind, h_ind, w_ind, ci_ind, h_K_ind, w_K_ind in T.grid(N, CO, OUT_H, OUT_W, CI, K, K):\n",
    "            with T.block(\"res\"):\n",
    "                v_N_ind, v_oc_ind, v_h_ind, v_w_ind, v_ci_ind, v_h_K_ind, v_w_K_ind = T.axis.remap(\"SSSSRRR\", [N_ind, oc_ind, h_ind, w_ind, ci_ind, h_K_ind, w_K_ind])\n",
    "                with T.init():\n",
    "                    res[v_N_ind, v_oc_ind, v_h_ind, v_w_ind] = T.int64(0)\n",
    "                res[v_N_ind, v_oc_ind, v_h_ind, v_w_ind] = res[v_N_ind, v_oc_ind, v_h_ind, v_w_ind] + data[v_N_ind, v_ci_ind, v_h_K_ind + v_h_ind, v_w_K_ind + v_w_ind] * weight[v_oc_ind, v_ci_ind, v_h_K_ind, v_w_K_ind]\n",
    "\n",
    "rt_lib = tvm.build(MyConv, target=\"llvm\")\n",
    "data_tvm = tvm.nd.array(data)\n",
    "weight_tvm = tvm.nd.array(weight)\n",
    "conv_tvm = tvm.nd.array(np.empty((N, CO, OUT_H, OUT_W), dtype=np.int64))\n",
    "rt_lib[\"conv\"](data_tvm, weight_tvm, conv_tvm)\n",
    "np.testing.assert_allclose(conv_tvm.numpy(), res_torch, rtol=1e-5)\n",
    "conv_tvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第二节：变换 IR\n",
    "在本节中，让我们尝试变换程序。我们在采用了 bmm_relu (batched_matmul_relu)，这是一种常见于 Transformer 等模型中的操作变体。\n",
    "\n",
    "首先，我们介绍一些新的原语：parallel、vectorize 和 unroll。这三个原语被应用于循环上，指示循环应当如何执行。这是示例："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @tir.prim_func\n",
      "    def func(A: tir.Buffer[(4, 4), \"int64\"], B: tir.Buffer[(4, 4), \"int64\"], C: tir.Buffer[(4, 4), \"int64\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"add\"})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        for i_0 in tir.parallel(2):\n",
      "            for i_1 in tir.unroll(2):\n",
      "                for j in tir.vectorized(4):\n",
      "                    with tir.block(\"C\"):\n",
      "                        vi = tir.axis.spatial(4, i_0 * 2 + i_1)\n",
      "                        vj = tir.axis.spatial(4, j)\n",
      "                        tir.reads(A[vi, vj], B[vi, vj])\n",
      "                        tir.writes(C[vi, vj])\n",
      "                        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyAdd:\n",
    "  @T.prim_func\n",
    "  def add(A: T.Buffer[(4, 4), \"int64\"],\n",
    "          B: T.Buffer[(4, 4), \"int64\"],\n",
    "          C: T.Buffer[(4, 4), \"int64\"]):\n",
    "    T.func_attr({\"global_symbol\": \"add\"})\n",
    "    for i, j in T.grid(4, 4):\n",
    "      with T.block(\"C\"):\n",
    "        vi = T.axis.spatial(4, i)\n",
    "        vj = T.axis.spatial(4, j)\n",
    "        C[vi, vj] = A[vi, vj] + B[vi, vj]\n",
    "\n",
    "sch = tvm.tir.Schedule(MyAdd)\n",
    "block = sch.get_block(\"C\", func_name=\"add\")\n",
    "i, j = sch.get_loops(block)\n",
    "i0, i1 = sch.split(i, factors=[2, 2])\n",
    "sch.parallel(i0)\n",
    "sch.unroll(i1)\n",
    "sch.vectorize(j)\n",
    "print(sch.mod.script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "变换批量矩阵乘法程序\n",
    "\n",
    "现在，让我们回到 bmm_relu 练习。首先，让我们看看 bmm 的定义：\n",
    "\n",
    "- $Y_{n,i,j} = \\sum_k A_{n,i,k} * B{n,k,j}$\n",
    "- $C_{n,i,j} =relu(Y_{n,i,j})$\n",
    "\n",
    "现在是你为 bmm_relu 编写 TensorIR 的时候了。我们提供 lnumpy 函数作为提示：\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnumpy_mm_relu_v2(A: np.ndarray, B: np.ndarray, C: np.ndarray):\n",
    "    Y = np.empty((16, 128, 128), dtype=\"float32\")\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                for k in range(128):\n",
    "                    if k == 0:\n",
    "                        Y[n, i, j] = 0\n",
    "                    Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "    for n in range(16):\n",
    "        for i in range(128):\n",
    "            for j in range(128):\n",
    "                C[n, i, j] = max(Y[n, i, j], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @tir.prim_func\n",
      "    def func(A: tir.Buffer[(16, 128, 128), \"float32\"], B: tir.Buffer[(16, 128, 128), \"float32\"], C: tir.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        Y = tir.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n, i, j, k in tir.grid(16, 128, 128, 128):\n",
      "            with tir.block(\"Y\"):\n",
      "                vn, vi, vj, vk = tir.axis.remap(\"SSSR\", [n, i, j, k])\n",
      "                tir.reads(A[vn, vi, vk], B[vn, vk, vj])\n",
      "                tir.writes(Y[vn, vi, vj])\n",
      "                with tir.init():\n",
      "                    Y[vn, vi, vj] = tir.float32(0)\n",
      "                Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "        for n, i, j in tir.grid(16, 128, 128):\n",
      "            with tir.block(\"C\"):\n",
      "                vn, vi, vj = tir.axis.remap(\"SSS\", [n, i, j])\n",
      "                tir.reads(Y[vn, vi, vj])\n",
      "                tir.writes(C[vn, vi, vj])\n",
      "                C[vn, vi, vj] = tir.max(Y[vn, vi, vj], tir.float32(0))\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "from numpy import dtype\n",
    "\n",
    "\n",
    "@tvm.script.ir_module\n",
    "class MyBmmRule:\n",
    "  @T.prim_func\n",
    "  def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "               B: T.Buffer[(16, 128, 128), \"float32\"],\n",
    "               C: T.Buffer[(16, 128, 128), \"float32\"]):\n",
    "    T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "    Y = T.alloc_buffer((16, 128, 128), dtype=\"float32\")\n",
    "    for n, i, j, k in T.grid(16, 128, 128, 128):\n",
    "        with T.block(\"Y\"):\n",
    "            vn, vi, vj, vk = T.axis.remap(\"SSSR\", [n, i, j, k])\n",
    "            with T.init():\n",
    "                Y[vn, vi, vj] = T.float32(0)\n",
    "            Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
    "\n",
    "    for n, i, j in T.grid(16, 128, 128):\n",
    "                with T.block(\"C\"):\n",
    "                    vn, vi, vj = T.axis.remap(\"SSS\", [n, i, j])\n",
    "                    C[vn, vi, vj] = T.max(Y[vn, vi, vj], T.float32(0))\n",
    "\n",
    "sch = tvm.tir.Schedule(MyBmmRule)\n",
    "print(sch.mod.script())\n",
    "# Also please validate your result\n",
    "\n",
    "mat_a = np.random.rand(16, 128, 128).astype(\"float32\")\n",
    "mat_b = np.random.rand(16, 128, 128).astype(\"float32\")\n",
    "mat_c_np = np.empty((16, 128, 128), dtype=\"float32\")\n",
    "lnumpy_mm_relu_v2(mat_a, mat_b, mat_c_np)\n",
    "\n",
    "\n",
    "mat_a_nd = tvm.nd.array(mat_a)\n",
    "mat_b_nd = tvm.nd.array(mat_b)\n",
    "mat_c_nd = tvm.nd.array(np.empty((16, 128, 128), dtype=\"float32\"))\n",
    "rt_lib = tvm.build(MyBmmRule, target=\"llvm\")\n",
    "func = rt_lib[\"bmm_relu\"]\n",
    "func(mat_a_nd, mat_b_nd, mat_c_nd)\n",
    "np.testing.assert_allclose(mat_c_nd.numpy(), mat_c_np, rtol=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本练习中，让我们专注于将原始程序变换为特定目标。请注意，由于硬件不同，目标程序可能不是最好的程序。但是这个练习旨在让你了解如何将程序变换为想要的程序。 这是目标程序：\n",
    "```python\n",
    "@tvm.script.ir_module\n",
    "class TargetModule:\n",
    "    @T.prim_func\n",
    "    def bmm_relu(A: T.Buffer[(16, 128, 128), \"float32\"], B: T.Buffer[(16, 128, 128), \"float32\"], C: T.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
    "        T.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
    "        for i0 in T.parallel(16):\n",
    "            for i1, i2_0 in T.grid(128, 16):\n",
    "                for ax0_init in T.vectorized(8):\n",
    "                    with T.block(\"Y_init\"):\n",
    "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                        j = T.axis.spatial(128, i2_0 * 8 + ax0_init)\n",
    "                        Y[n, i, j] = T.float32(0)\n",
    "                for ax1_0 in T.serial(32):\n",
    "                    for ax1_1 in T.unroll(4):\n",
    "                        for ax0 in T.serial(8):\n",
    "                            with T.block(\"Y_update\"):\n",
    "                                n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                                j = T.axis.spatial(128, i2_0 * 8 + ax0)\n",
    "                                k = T.axis.reduce(128, ax1_0 * 4 + ax1_1)\n",
    "                                Y[n, i, j] = Y[n, i, j] + A[n, i, k] * B[n, k, j]\n",
    "                for i2_1 in T.vectorized(8):\n",
    "                    with T.block(\"C\"):\n",
    "                        n, i = T.axis.remap(\"SS\", [i0, i1])\n",
    "                        j = T.axis.spatial(128, i2_0 * 8 + i2_1)\n",
    "                        C[n, i, j] = T.max(Y[n, i, j], T.float32(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @tir.prim_func\n",
      "    def func(A: tir.Buffer[(16, 128, 128), \"float32\"], B: tir.Buffer[(16, 128, 128), \"float32\"], C: tir.Buffer[(16, 128, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"bmm_relu\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        Y = tir.alloc_buffer([16, 128, 128], dtype=\"float32\")\n",
      "        for n, i, j_0 in tir.grid(16, 128, 16):\n",
      "            for j_1_init in tir.vectorized(8):\n",
      "                with tir.block(\"Y_init\"):\n",
      "                    vn, vi = tir.axis.remap(\"SS\", [n, i])\n",
      "                    vj = tir.axis.spatial(128, j_0 * 8 + j_1_init)\n",
      "                    tir.reads()\n",
      "                    tir.writes(Y[vn, vi, vj])\n",
      "                    Y[vn, vi, vj] = tir.float32(0)\n",
      "            for k_0, k_1, j_1 in tir.grid(32, 4, 8):\n",
      "                with tir.block(\"Y_update\"):\n",
      "                    vn, vi = tir.axis.remap(\"SS\", [n, i])\n",
      "                    vj = tir.axis.spatial(128, j_0 * 8 + j_1)\n",
      "                    vk = tir.axis.reduce(128, k_0 * 4 + k_1)\n",
      "                    tir.reads(Y[vn, vi, vj], A[vn, vi, vk], B[vn, vk, vj])\n",
      "                    tir.writes(Y[vn, vi, vj])\n",
      "                    Y[vn, vi, vj] = Y[vn, vi, vj] + A[vn, vi, vk] * B[vn, vk, vj]\n",
      "            for ax0 in tir.serial(8):\n",
      "                with tir.block(\"C\"):\n",
      "                    vn, vi = tir.axis.remap(\"SS\", [n, i])\n",
      "                    vj = tir.axis.spatial(128, j_0 * 8 + ax0)\n",
      "                    tir.reads(Y[vn, vi, vj])\n",
      "                    tir.writes(C[vn, vi, vj])\n",
      "                    C[vn, vi, vj] = tir.max(Y[vn, vi, vj], tir.float32(0))\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "sch = tvm.tir.Schedule(MyBmmRule)\n",
    "# TODO: transformations\n",
    "# Hints: you can use\n",
    "# `IPython.display.Code(sch.mod.script(), language=\"python\")`\n",
    "# or `print(sch.mod.script())`\n",
    "# to show the current program at any time during the transformation.\n",
    "\n",
    "# Step 1. Get blocks\n",
    "Y = sch.get_block(\"Y\", func_name=\"bmm_relu\")\n",
    "C = sch.get_block(\"C\", func_name=\"bmm_relu\")\n",
    "\n",
    "# Step 2. Get loops\n",
    "b, i, j, k = sch.get_loops(Y)\n",
    "\n",
    "# Step 3. Organize the loops\n",
    "k0, k1 = sch.split(k, [None, 4])\n",
    "j0, j1 = sch.split(j, [None, 8])\n",
    "\n",
    "sch.reorder(j0, k0, k1, j1)\n",
    "\n",
    "sch.reverse_compute_at(C, j0)\n",
    "\n",
    "# Step 4. decompose reduction\n",
    "Y_init = sch.decompose_reduction(Y, k0)\n",
    "_, _, _, y_init = sch.get_loops(Y_init)\n",
    "\n",
    "# Step 5. vectorize / parallel / unroll\n",
    "sch.vectorize(y_init)\n",
    "# sch.parallel(b)\n",
    "# sch.unroll(...)\n",
    "print(sch.mod.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<built-in method __dir__ of method-wrapper object at 0x7f7eca16ad10>\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f5cc334afce0b89effac1c4034e2a2d057ab73bae343e6f18f6adb4c89091e1a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

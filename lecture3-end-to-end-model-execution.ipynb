{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# end to end model execution\n",
    "\n",
    "怎么样将机器学习模型的张量函数整合到计算图中\n",
    "\n",
    "将使用一真实的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import tvm\n",
    "from tvm.ir.module import IRModule\n",
    "from tvm.script import tir as T\n",
    "from tvm.script import relax as R\n",
    "import numpy as np\n",
    "from tvm import relax\n",
    "# This is needed for deferring annotation parsing in TVMScript\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load dataset\n",
    "fashion MNIST dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "import torch\n",
    "test_data = torchvision.datasets.FashionMNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=torchvision.transforms.ToTensor(),\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=1, shuffle=True)\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "               \n",
    "img, label = next(iter(test_loader))\n",
    "img = img.reshape(1, 28, 28).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class: Sneaker\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure() \n",
    "plt.imshow(img[0])\n",
    "plt.colorbar()\n",
    "plt.grid(False)\n",
    "plt.show()\n",
    "\n",
    "print(\"Class:\", class_names[label[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load model parameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2022-07-11 15:28:06--  https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl\n",
      "正在解析主机 github.com (github.com)... 20.205.243.166\n",
      "正在连接 github.com (github.com)|20.205.243.166|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 302 Found\n",
      "位置：https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl [跟随至新的 URL]\n",
      "--2022-07-11 15:28:06--  https://raw.githubusercontent.com/mlc-ai/web-data/main/models/fasionmnist_mlp_params.pkl\n",
      "正在解析主机 raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8002::154, 2606:50c0:8003::154, ...\n",
      "正在连接 raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... 已连接。\n",
      "已发出 HTTP 请求，正在等待回应... 200 OK\n",
      "长度： 407396 (398K) [application/octet-stream]\n",
      "正在保存至: “fasionmnist_mlp_params.pkl.1”\n",
      "\n",
      "fasionmnist_mlp_par 100%[===================>] 397.85K  1.15MB/s    用时 0.3s    \n",
      "\n",
      "2022-07-11 15:28:08 (1.15 MB/s) - 已保存 “fasionmnist_mlp_params.pkl.1” [407396/407396])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget https://github.com/mlc-ai/web-data/raw/main/models/fasionmnist_mlp_params.pkl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 端到端模型整合\n",
    "两层神经网络(这里省略了 softmax)\n",
    "- linear -> relu -> linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy 实现\n",
    "def numpy_mlp(data, w0, b0, w1, b1):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        data (np.ndarray): data shape: (batch size, input size)\n",
    "        w0 (np.ndarray): data shape: (input size, hidden size)\n",
    "        b0 (np.ndarray): data shape: (hidden size, )\n",
    "        w1 (np.ndarray): data shape: (hidden size, class num)\n",
    "        b1 (np.ndarray): data shape: (class num)\n",
    "    \"\"\"\n",
    "    lv0 = data @ w0.T + b0\n",
    "    lv1 = np.maximum(lv0, 0)\n",
    "    lv2 = lv1 @ w1.T + b1\n",
    "    return lv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-16.835085  -31.70045   -21.136889  -24.41916   -30.072502   -3.3631787\n",
      "  -24.313276   19.233906  -13.347248   -5.056448 ]]\n",
      "[7]\n",
      "Numpy-MLP Prediction: Sneaker\n"
     ]
    }
   ],
   "source": [
    "import pickle as pkl\n",
    "mlp_params = pkl.load(open(\"fasionmnist_mlp_params.pkl\", \"rb\"))\n",
    "res = numpy_mlp(\n",
    "    data = img.reshape(1, 784),\n",
    "    w0 = mlp_params[\"w0\"],\n",
    "    b0 = mlp_params[\"b0\"],\n",
    "    w1 = mlp_params[\"w1\"],\n",
    "    b1 = mlp_params[\"b1\"]\n",
    ")\n",
    "\n",
    "print(res)\n",
    "pred_kind = res.argmax(axis=1)\n",
    "print(pred_kind)\n",
    "print(\"Numpy-MLP Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## low level numpy 实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-16.835081  -31.700447  -21.136889  -24.419159  -30.072495   -3.3631804\n",
      "  -24.31327    19.233908  -13.347247   -5.056448 ]]\n",
      "Low-level Numpy MLP Prediction: Sneaker\n"
     ]
    }
   ],
   "source": [
    "def lnumpy_linear0(X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): shape (1, 784)\n",
    "        W (np.ndarray): shape (784, 128)\n",
    "        B (np.ndarray): shape (128, )\n",
    "        Z (np.ndarray): (1, 128))\n",
    "    \"\"\"\n",
    "    Y = np.empty((1, 128), dtype=\"float32\")\n",
    "    for i in range(1):\n",
    "        for j in range(128):\n",
    "            for k in range(784):\n",
    "                if k == 0:\n",
    "                    Y[i, j] = 0\n",
    "                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]\n",
    "    \n",
    "    for i in range(1):\n",
    "        for j in range(128):\n",
    "            Z[i, j] = Y[i, j] + B[j]\n",
    "\n",
    "def lnumpy_relu0(X: np.ndarray, Y: np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): shape (1, 128)\n",
    "        Y (np.ndarray): shape (1, 128)\n",
    "    \"\"\"\n",
    "    for i in range(1):\n",
    "        for j in range(128):\n",
    "            Y[i, j] = np.maximum(X[i, j], 0)\n",
    "\n",
    "def lnumpy_linear1(X: np.ndarray, W: np.ndarray, B: np.ndarray, Z: np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): shape (1, 128)\n",
    "        W (np.ndarray): shape (128, 10)\n",
    "        B (np.ndarray): shape (10, )\n",
    "        Z (np.ndarray): (1, 10))\n",
    "    \"\"\"\n",
    "    Y = np.empty((1, 10), dtype=\"float32\") \n",
    "    for i in range(1):\n",
    "        for j in range(10):\n",
    "            for k in range(128):\n",
    "                if k == 0:\n",
    "                    Y[i, j] = 0\n",
    "                Y[i, j] = Y[i, j] + X[i, k] * W[j, k]\n",
    "\n",
    "    for i in range(1):\n",
    "        for j in range(10):\n",
    "            Z[i, j] = Y[i, j] + B[j]\n",
    "\n",
    "# main\n",
    "def lnumpy_mlp(X: np.ndarray,\n",
    "               w0: np.ndarray,\n",
    "               b0: np.ndarray,\n",
    "               w1: np.ndarray,\n",
    "               b1: np.ndarray):\n",
    "    \"\"\"_summary_\n",
    "\n",
    "    Args:\n",
    "        X (np.ndarray): shape (1, 784)\n",
    "        w0 (np.ndarray): shape (784, 128)\n",
    "        b0 (np.ndarray): shape (128, )\n",
    "        w1 (np.ndarray): shape (128, 10)\n",
    "        b1 (np.ndarray): shape (10, )\n",
    "    \"\"\"\n",
    "    # 中间结果 lv0\n",
    "    lv0 = np.empty((1, 128), dtype=\"float32\")\n",
    "    lnumpy_linear0(X, w0, b0, lv0)\n",
    "    # 中间结果 lv1\n",
    "    lv1 = np.empty((1, 128), dtype=\"float32\")\n",
    "    lnumpy_relu0(lv0, lv1)\n",
    "\n",
    "    out = np.empty((1, 10), dtype=\"float32\")\n",
    "    lnumpy_linear1(lv1, w1, b1, out)\n",
    "    return out\n",
    "\n",
    "result = lnumpy_mlp(img.reshape(1, 784),\n",
    "                    mlp_params[\"w0\"],\n",
    "                    mlp_params[\"b0\"],\n",
    "                    mlp_params[\"w1\"],\n",
    "                    mlp_params[\"b1\"])\n",
    "print(result)\n",
    "pred_kind = result.argmax(axis=1)\n",
    "print(\"Low-level Numpy MLP Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct end to end in TVMScript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModule:\n",
    "    @T.prim_func\n",
    "    def linear0(X: T.Buffer[(1, 784), \"float32\"],\n",
    "                W0: T.Buffer[(128, 784),\"float32\"],\n",
    "                B0: T.Buffer[(128, ), \"float32\"],\n",
    "                Z: T.Buffer[(1, 128), \"float32\"]):\n",
    "\n",
    "        T.func_attr({\"global_symbol\": \"linear0\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer((1, 128), \"float32\")\n",
    "        for i, j, k in T.grid(1, 128, 784):\n",
    "            with T.block(\"Y\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = 0\n",
    "                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W0[vj, vk]\n",
    "\n",
    "        for i, j in T.grid(1, 128):\n",
    "            with T.block(\"Z\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "                Z[vi, vj] = Y[vi, vj] + B0[vj] \n",
    "\n",
    "    @T.prim_func\n",
    "    def relu0(X: T.Buffer[(1, 128), \"float32\"],\n",
    "              Y: T.Buffer[(1, 128), \"float32\"]):\n",
    "\n",
    "        T.func_attr({\"global_symbol\": \"relu0\", \"tir.noalias\": True})\n",
    "        for i, j in T.grid(1, 128):\n",
    "            with T.block(\"Y\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "                Y[vi, vj] = T.max(X[vi, vj], T.float32(0))\n",
    "    \n",
    "    @T.prim_func\n",
    "    def linear1(X: T.Buffer[(1, 128), \"float32\"], \n",
    "                W: T.Buffer[(10, 128), \"float32\"], \n",
    "                B: T.Buffer[(10, ), \"float32\"], \n",
    "                Z: T.Buffer[(1, 10), \"float32\"]):\n",
    "                \n",
    "        T.func_attr({\"global_symbol\": \"linear1\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer((1, 10), \"float32\")\n",
    "        for i, j, k in T.grid(1, 10, 128):\n",
    "            with T.block(\"Y\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = T.float32(0)\n",
    "                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]\n",
    "    \n",
    "        for i, j in T.grid(1, 10):\n",
    "            with T.block(\"Z\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "                Z[vi, vj] = Y[vi, vj] + B[vj]\n",
    "    \n",
    "    @R.function\n",
    "    def main(x: Tensor((1, 784), \"float32\"), \n",
    "             w0: Tensor((128, 784), \"float32\"), \n",
    "             b0: Tensor((128, ), \"float32\"), \n",
    "             w1: Tensor((10, 128), \"float32\"), \n",
    "             b1: Tensor((10, ), \"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv0 = R.call_tir(linear0, (x, w0, b0), (1, 128), dtype=\"float32\")\n",
    "            lv1 = R.call_tir(relu0, (lv0), (1, 128), dtype=\"float32\")\n",
    "            out = R.call_tir(linear1, (lv1, w1, b1), (1, 10), dtype=\"float32\")\n",
    "            R.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意这里每一个张量函数都需要将输出都当做函数的参数传入，但是我们在上层的计算图中希望张量函数通过返回值的方式返回\n",
    "\n",
    "因此 tvm 这里提出目标传递 (destination passing) 的概念。这个想法是输入和输出在外部显式分配并传递给底层元函数。 这种风格通常用于底层库设计，因此高层框架可以处理内存分配决策。 请注意，并非所有张量操作都可以通过这种方式呈现（比如，有些操作的输出形状取决于输入）。 然而，在通常的实践中，如果可能的话，以这种风格编写底层函数通常是有帮助的。\n",
    "\n",
    "虽然可以通过显式分配中间结果并调用每个函数来将目标传递的函数组装在一起，但很难将以下代码转换为计算图形式。\n",
    "\n",
    "下面为 `call_tir` 的一种实现，在实际应用中，可以有不同的底层方法来优化执行。 例如，我们可能会选择提前分配所有输出内存，然后运行，我们将在以后的课程中介绍。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lnumpy_call_tir(prim_func, inputs, out_shape, dtype):\n",
    "    res = np.empty(out_shape, dtype=dtype)\n",
    "    prim_func(*inputs, res)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们还可以在底层 NumPy 中看到 `call_tir` 的作用。 现在我们已经定义了 `lnumpy_call_tir`，我们可以将底层 NumPy 代码重写为："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Low-level Numpy with CallTIR Prediction: Sneaker\n"
     ]
    }
   ],
   "source": [
    "def lnumpy_mlp(data, w0, b0, w1, b1):\n",
    "    lv0 = lnumpy_call_tir(lnumpy_linear0, (data, w0, b0), (1, 128), dtype=\"float32\")\n",
    "    lv1 = lnumpy_call_tir(lnumpy_relu0, (lv0, ), (1, 128), dtype=\"float32\")\n",
    "    out = lnumpy_call_tir(lnumpy_linear1, (lv1, w1, b1), (1, 10), dtype=\"float32\")\n",
    "    return out\n",
    "\n",
    "res = lnumpy_mlp(img.reshape(1, -1),\n",
    "                 w0=mlp_params[\"w0\"],\n",
    "                 b0=mlp_params[\"b0\"],\n",
    "                 w1=mlp_params[\"w1\"],\n",
    "                 b1=mlp_params[\"b1\"])\n",
    "print(\"Low-level Numpy with CallTIR Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataflow block"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 构建并运行"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @tir.prim_func\n",
      "    def linear0(X: tir.Buffer[(1, 784), \"float32\"], W0: tir.Buffer[(128, 784), \"float32\"], B0: tir.Buffer[128, \"float32\"], Z: tir.Buffer[(1, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"linear0\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        Y = tir.alloc_buffer([1, 128], dtype=\"float32\")\n",
      "        for i, j, k in tir.grid(1, 128, 784):\n",
      "            with tir.block(\"Y\"):\n",
      "                vi, vj, vk = tir.axis.remap(\"SSR\", [i, j, k])\n",
      "                tir.reads(X[vi, vk], W0[vj, vk])\n",
      "                tir.writes(Y[vi, vj])\n",
      "                with tir.init():\n",
      "                    Y[vi, vj] = 0\n",
      "                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W0[vj, vk]\n",
      "        for i, j in tir.grid(1, 128):\n",
      "            with tir.block(\"Z\"):\n",
      "                vi, vj = tir.axis.remap(\"SS\", [i, j])\n",
      "                tir.reads(Y[vi, vj], B0[vj])\n",
      "                tir.writes(Z[vi, vj])\n",
      "                Z[vi, vj] = Y[vi, vj] + B0[vj]\n",
      "    \n",
      "    @relax.function\n",
      "    def main(x: Tensor((1, 784), \"float32\"), w0: Tensor((128, 784), \"float32\"), b0: Tensor((128,), \"float32\"), w1: Tensor((10, 128), \"float32\"), b1: Tensor((10,), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
      "        # block 0\n",
      "        with relax.dataflow():\n",
      "            lv0 = relax.call_tir(linear0, (x, w0, b0), (1, 128), dtype=\"float32\")\n",
      "            lv1 = relax.call_tir(relu0, lv0, (1, 128), dtype=\"float32\")\n",
      "            out = relax.call_tir(linear1, (lv1, w1, b1), (1, 10), dtype=\"float32\")\n",
      "            relax.output(out)\n",
      "        return out\n",
      "    \n",
      "    @tir.prim_func\n",
      "    def linear1(X: tir.Buffer[(1, 128), \"float32\"], W: tir.Buffer[(10, 128), \"float32\"], B: tir.Buffer[10, \"float32\"], Z: tir.Buffer[(1, 10), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"linear1\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        Y = tir.alloc_buffer([1, 10], dtype=\"float32\")\n",
      "        for i, j, k in tir.grid(1, 10, 128):\n",
      "            with tir.block(\"Y\"):\n",
      "                vi, vj, vk = tir.axis.remap(\"SSR\", [i, j, k])\n",
      "                tir.reads(X[vi, vk], W[vj, vk])\n",
      "                tir.writes(Y[vi, vj])\n",
      "                with tir.init():\n",
      "                    Y[vi, vj] = tir.float32(0)\n",
      "                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]\n",
      "        for i, j in tir.grid(1, 10):\n",
      "            with tir.block(\"Z\"):\n",
      "                vi, vj = tir.axis.remap(\"SS\", [i, j])\n",
      "                tir.reads(Y[vi, vj], B[vj])\n",
      "                tir.writes(Z[vi, vj])\n",
      "                Z[vi, vj] = Y[vi, vj] + B[vj]\n",
      "    \n",
      "    @tir.prim_func\n",
      "    def relu0(X: tir.Buffer[(1, 128), \"float32\"], Y: tir.Buffer[(1, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"relu0\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        for i, j in tir.grid(1, 128):\n",
      "            with tir.block(\"Y\"):\n",
      "                vi, vj = tir.axis.remap(\"SS\", [i, j])\n",
      "                tir.reads(X[vi, vj])\n",
      "                tir.writes(Y[vi, vj])\n",
      "                Y[vi, vj] = tir.max(X[vi, vj], tir.float32(0))\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(MyModule.script())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tvm.relax.vm.Executable"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex = relax.vm.build(MyModule, target=\"llvm\")\n",
    "type(ex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "build 函数会给我们一个可执行文件（译者注：“可执行文件”并非传统操作系统中的可执行文件，不能直接在系统中运行，而是针对Relax VM设计的一种文件格式）。 我们可以初始化一个虚拟机执行器，使我们能够运行该函数。 此外，我们将传入第二个参数，指示我们要在哪个设备上运行端到端执行。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vm = relax.VirtualMachine(ex, tvm.cpu())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "现在我们准备好运行模型了。 我们首先构建包含输入数据和权重的 tvm NDArray。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_nd = tvm.nd.array(img.reshape(1, 784))\n",
    "nd_params = {k: tvm.nd.array(v) for k, v in mlp_params.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-16.835081  -31.700447  -21.136889  -24.419159  -30.072495   -3.3631804\n",
      "  -24.31327    19.233908  -13.347247   -5.056448 ]]\n"
     ]
    }
   ],
   "source": [
    "nd_res = vm[\"main\"](data_nd, nd_params[\"w0\"], nd_params[\"b0\"], nd_params[\"w1\"], nd_params[\"b1\"])\n",
    "print(nd_res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 在环境中集成现有的库\n",
    "在许多情况下，我们可能希望将现有的库函数集成到 MLC 过程中。\n",
    "\n",
    "下面的 IRModule 展示了如何做到这一点：\n",
    "\n",
    "这些字符串是我们期望在模型执行期间的运行时函数 (runtime function) 的名称。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModuleWithExternCall:\n",
    "    @R.function\n",
    "    def main(x: Tensor((1, 784), \"float32\"),\n",
    "             w0: Tensor((128, 784), \"float32\"),\n",
    "             b0: Tensor((128,), \"float32\"),\n",
    "             w1: Tensor((10, 128), \"float32\"),\n",
    "             b1: Tensor((10,), \"float32\")):\n",
    "        # block 0\n",
    "        with R.dataflow():\n",
    "            lv0 = R.call_tir(\"env.linear\", (x, w0, b0), (1, 128), dtype=\"float32\")\n",
    "            lv1 = R.call_tir(\"env.relu\", (lv0,), (1, 128), dtype=\"float32\")\n",
    "            out = R.call_tir(\"env.linear\", (lv1, w1, b1), (1, 10), dtype=\"float32\")\n",
    "            R.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面实现 env.linear\n",
    "\n",
    "注意：\n",
    "- 在上面的代码中，我们使用 from_dlpack 将 TVM NDArray 转换为 torch NDArray。 请注意，这是一个零拷贝转换，这意味着 Torch 阵列与 TVM NDArray 共享底层内存。\n",
    "- DLPack 是一种通用的交换标准，允许不同的框架交换 Tensor/NDArray 而无需参与数据复制。 from_dlpack API 由多个框架支持，是 Python 数组 API 标准的一部分。\n",
    "\n",
    "在这个特定的函数中，我们只是简单地调用 PyTorch 的实现。 在真实的应用场景中，我们可以使用类似的机制将调用重定向到特定的库，例如 cuDNN 或我们自己的库实现。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@tvm.register_func(\"env.linear\", override=True)\n",
    "def torch_linear(x: tvm.nd.NDArray,\n",
    "                 w: tvm.nd.NDArray,\n",
    "                 b: tvm.nd.NDArray,\n",
    "                 out: tvm.nd.NDArray):\n",
    "    x_torch = torch.from_dlpack(x)\n",
    "    w_torch = torch.from_dlpack(w)\n",
    "    b_torch = torch.from_dlpack(b)\n",
    "    out_torch = torch.from_dlpack(out)\n",
    "\n",
    "    torch.mm(x_torch, w_torch.T, out=out_torch)\n",
    "    torch.add(out_torch, b_torch, out=out_torch)\n",
    "\n",
    "@tvm.register_func(\"env.relu\", override=True)\n",
    "def torch_relu(x: tvm.nd.NDArray, out: tvm.nd.NDArray):\n",
    "    x_torch = torch.from_dlpack(x)\n",
    "    out_torch = torch.from_dlpack(out)\n",
    "    torch.max(x_torch, torch.Tensor([0.0]), out=out_torch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModuleWithExternCall Prediction: Sneaker\n"
     ]
    }
   ],
   "source": [
    "ex = relax.vm.build(MyModuleWithExternCall, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "nd_res = vm[\"main\"](data_nd,\n",
    "                    nd_params[\"w0\"],\n",
    "                    nd_params[\"b0\"],\n",
    "                    nd_params[\"w1\"],\n",
    "                    nd_params[\"b1\"])\n",
    "\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MyModuleWithExternCall Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixing TensorIR Code and Libraries\n",
    "\n",
    "在上一个示例中，我们构建了一个 IRModule，其中所有元操作都被分派给运行库。有时将两者混合使用会有所帮助。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tvm.script.ir_module\n",
    "class MyModuleMixture:\n",
    "    @T.prim_func\n",
    "    def linear0(X: T.Buffer[(1, 784), \"float32\"],\n",
    "                W: T.Buffer[(128, 784), \"float32\"],\n",
    "                B: T.Buffer[(128,), \"float32\"],\n",
    "                Z: T.Buffer[(1, 128), \"float32\"]):\n",
    "        T.func_attr({\"global_symbol\": \"linear0\", \"tir.noalias\": True})\n",
    "        Y = T.alloc_buffer((1, 128), \"float32\")\n",
    "        for i, j, k in T.grid(1, 128, 784):\n",
    "            with T.block(\"Y\"):\n",
    "                vi, vj, vk = T.axis.remap(\"SSR\", [i, j, k])\n",
    "                with T.init():\n",
    "                    Y[vi, vj] = T.float32(0)\n",
    "                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]\n",
    "\n",
    "        for i, j in T.grid(1, 128):\n",
    "            with T.block(\"Z\"):\n",
    "                vi, vj = T.axis.remap(\"SS\", [i, j])\n",
    "                Z[vi, vj] =  Y[vi, vj] + B[vj]\n",
    "\n",
    "    @R.function\n",
    "    def main(x: Tensor((1, 784), \"float32\"),\n",
    "             w0: Tensor((128, 784), \"float32\"),\n",
    "             b0: Tensor((128,), \"float32\"),\n",
    "             w1: Tensor((10, 128), \"float32\"),\n",
    "             b1: Tensor((10,), \"float32\")):\n",
    "        with R.dataflow():\n",
    "            lv0 = R.call_tir(linear0, (x, w0, b0), (1, 128), dtype=\"float32\")\n",
    "            lv1 = R.call_tir(\"env.relu\", (lv0,), (1, 128), dtype=\"float32\")\n",
    "            out = R.call_tir(\"env.linear\", (lv1, w1, b1), (1, 10), dtype=\"float32\")\n",
    "            R.output(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "上面的代码块显示了一个示例，其中 linear0 仍然在 TensorIR 中实现，而其余函数被重定向到库函数。 我们可以构建并运行以验证结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModuleMixture Prediction: Sneaker\n"
     ]
    }
   ],
   "source": [
    "ex = relax.vm.build(MyModuleMixture, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "nd_res = vm[\"main\"](data_nd,\n",
    "                    nd_params[\"w0\"],\n",
    "                    nd_params[\"b0\"],\n",
    "                    nd_params[\"w1\"],\n",
    "                    nd_params[\"b1\"])\n",
    "\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MyModuleMixture Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 将参数绑定到 IRModule\n",
    "在到目前为止的所有示例中，我们通过显式传递参数来构造主函数。 在许多情况下，将参数绑定为附加到 IRModule 的常量通常会降低API的复杂程度。 以下代码通过将参数名称与 nd_params 中的键匹配来创建绑定。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @relax.function\n",
      "    def main(x: Tensor((1, 784), \"float32\"), w0: Tensor((128, 784), \"float32\"), b0: Tensor((128,), \"float32\"), w1: Tensor((10, 128), \"float32\"), b1: Tensor((10,), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
      "        # block 0\n",
      "        with relax.dataflow():\n",
      "            lv0 = relax.call_tir(linear0, (x, w0, b0), (1, 128), dtype=\"float32\")\n",
      "            lv1 = relax.call_tir(\"env.relu\", (lv0,), (1, 128), dtype=\"float32\")\n",
      "            out = relax.call_tir(\"env.linear\", (lv1, w1, b1), (1, 10), dtype=\"float32\")\n",
      "            relax.output(out)\n",
      "        return out\n",
      "    \n",
      "    @tir.prim_func\n",
      "    def linear0(X: tir.Buffer[(1, 784), \"float32\"], W: tir.Buffer[(128, 784), \"float32\"], B: tir.Buffer[128, \"float32\"], Z: tir.Buffer[(1, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"linear0\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        Y = tir.alloc_buffer([1, 128], dtype=\"float32\")\n",
      "        for i, j, k in tir.grid(1, 128, 784):\n",
      "            with tir.block(\"Y\"):\n",
      "                vi, vj, vk = tir.axis.remap(\"SSR\", [i, j, k])\n",
      "                tir.reads(X[vi, vk], W[vj, vk])\n",
      "                tir.writes(Y[vi, vj])\n",
      "                with tir.init():\n",
      "                    Y[vi, vj] = tir.float32(0)\n",
      "                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]\n",
      "        for i, j in tir.grid(1, 128):\n",
      "            with tir.block(\"Z\"):\n",
      "                vi, vj = tir.axis.remap(\"SS\", [i, j])\n",
      "                tir.reads(Y[vi, vj], B[vj])\n",
      "                tir.writes(Z[vi, vj])\n",
      "                Z[vi, vj] = Y[vi, vj] + B[vj]\n",
      "    \n",
      "------------------------------------\n",
      "@tvm.script.ir_module\n",
      "class Module:\n",
      "    @relax.function\n",
      "    def main(x: Tensor((1, 784), \"float32\")) -> Tensor(None, \"float32\", ndim = 2):\n",
      "        # block 0\n",
      "        with relax.dataflow():\n",
      "            lv0 = relax.call_tir(linear0, (x, meta[relay.Constant][0], meta[relay.Constant][1]), (1, 128), dtype=\"float32\")\n",
      "            lv1 = relax.call_tir(\"env.relu\", (lv0,), (1, 128), dtype=\"float32\")\n",
      "            out = relax.call_tir(\"env.linear\", (lv1, meta[relay.Constant][2], meta[relay.Constant][3]), (1, 10), dtype=\"float32\")\n",
      "            relax.output(out)\n",
      "        return out\n",
      "    \n",
      "    @tir.prim_func\n",
      "    def linear0(X: tir.Buffer[(1, 784), \"float32\"], W: tir.Buffer[(128, 784), \"float32\"], B: tir.Buffer[128, \"float32\"], Z: tir.Buffer[(1, 128), \"float32\"]) -> None:\n",
      "        # function attr dict\n",
      "        tir.func_attr({\"global_symbol\": \"linear0\", \"tir.noalias\": True})\n",
      "        # body\n",
      "        # with tir.block(\"root\")\n",
      "        Y = tir.alloc_buffer([1, 128], dtype=\"float32\")\n",
      "        for i, j, k in tir.grid(1, 128, 784):\n",
      "            with tir.block(\"Y\"):\n",
      "                vi, vj, vk = tir.axis.remap(\"SSR\", [i, j, k])\n",
      "                tir.reads(X[vi, vk], W[vj, vk])\n",
      "                tir.writes(Y[vi, vj])\n",
      "                with tir.init():\n",
      "                    Y[vi, vj] = tir.float32(0)\n",
      "                Y[vi, vj] = Y[vi, vj] + X[vi, vk] * W[vj, vk]\n",
      "        for i, j in tir.grid(1, 128):\n",
      "            with tir.block(\"Z\"):\n",
      "                vi, vj = tir.axis.remap(\"SS\", [i, j])\n",
      "                tir.reads(Y[vi, vj], B[vj])\n",
      "                tir.writes(Z[vi, vj])\n",
      "                Z[vi, vj] = Y[vi, vj] + B[vj]\n",
      "    \n"
     ]
    }
   ],
   "source": [
    "print(MyModuleMixture.script())\n",
    "MyModuleWithParams = relax.transform.BindParams(\"main\", nd_params)(MyModuleMixture)\n",
    "print(\"------------------------------------\")\n",
    "print(MyModuleWithParams.script())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在上面的脚本中，`meta[relay.Constant][0]` （译者注：目前 Relax 的常量表达依然继承自 Relay ，未来改API可能会更改） 对应于一个存储常量的隐式字典（它没有显示为脚本的一部分，但仍然是 IRModule 的一部分）。 如果我们构建转换后的 IRModule，我们现在可以通过传入输入数据来调用该函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyModuleWithParams Prediction: Sneaker\n"
     ]
    }
   ],
   "source": [
    "ex = relax.vm.build(MyModuleWithParams, target=\"llvm\")\n",
    "vm = relax.VirtualMachine(ex, tvm.cpu())\n",
    "\n",
    "nd_res = vm[\"main\"](data_nd)\n",
    "\n",
    "pred_kind = np.argmax(nd_res.numpy(), axis=1)\n",
    "print(\"MyModuleWithParams Prediction:\", class_names[pred_kind[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('mlc-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a537d536684414c640a606739ee5e008de51f3cfc4afd56de838724fee6bb8e9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
